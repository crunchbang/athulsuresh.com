[{"content":"This is my review of book 1: It sucks!\nBooo!\n","permalink":"//localhost:1313/posts/books/1-book1/","summary":"\u003cp\u003eThis is my review of book 1: It sucks!\u003c/p\u003e\n\u003cp\u003eBooo!\u003c/p\u003e","title":"Book 1"},{"content":"Collection of unstructed notes that I accumulated while learning Erlang. This is a work-in-progress.\nerlang programs are composed on communicating processing. Like modeling objects, processes should be modeled to fit the problem. This is called modeling concurrency. concurrency is about structure. parallelism is about execution. each expression must end with a . variables in erlang can only be bound once. Variables start with UPPERCASE letters. atoms begin with lowercase letters. processes evaluate fns that are defined in modules. Modules are files with .erl extension. pid ! {client_pid, message} for sending message to pid. c(module_name) to compile the module. f() to forget the bindings. file server and client. client provides the abstraction and hides the details of communication with the actual process. This gives use the flexibility to change the underlying implementation without changing the interface exposed through the client. It\u0026rsquo;s refreshing how Joe Armstrong doesn\u0026rsquo;t talk down to the programmer in this book. The innards and complicated lingua franca is exposed for everyone to see. erlang can handle arbitrary precision numbers. Like really big numbers. = is more of a pattern matching operator rather than assignment operator. atoms are similar to symbolic constants or enums. atoms are global. atoms can be quoted and can have spaces in them. {item1, blah} represents a tuple of fixed size. Since tuples don\u0026rsquo;t have type, it\u0026rsquo;s a convention to add an atom as the first element indicating the type. {point, 0, 1}. {point, C, C} = {point, 25, 25} works! strings are represented as a list of integers with each int representing an unicode point. fullstop separates expression. comma separates related subordinate clauses. semicolon separates clauses. fun(arg) -\u0026gt; body end. to define anonymous fns. [f(X) || X \u0026lt;- L] list comprehension. X \u0026lt;- L follows the pattern logic for =. Named function F/n should be passed in as fun F/n when used as an argument -record(Name { field1 = DefaultValue1, .... fieldN }. undefined is the default value. .hrl files are like C header files where common definitions can be kept. #Name{key1=val1...}. to instantiate the struct. rr(\u0026quot;record_file.hrl\u0026quot;) to bring it into the erl. X#Name{field1=NewValue} to create a new record from an existing record X with a field value changed. Maps are weird. #{ key Op val, key2 Op val2}. := for updating an existing key. =\u0026gt; for adding a new key. The update follows the same pattern as records. ; is OR and , is AND for guard sequences. maps : get/find etc for acessing map values. clauses of a fn need to be separated by a SEMICOLON instead of a PERIOD! \u0026lt;\u0026lt;\u0026quot;binaries\u0026quot;\u0026gt;\u0026gt;. Binary values must be in the range 0-255. Any other value would wrap around and be mapped to a value in the 0-255 range. term_to_binary and binary_to_term. a type is binary when it\u0026rsquo;s size is divisible by 8, otherwise it\u0026rsquo;s a bitstring. \u0026lt;\u0026lt;R:5, B:6, G:5\u0026gt;\u0026gt; to pack elements into a binary while specifying the bit size. The same pattern HAS to be used when unpacking. type test BIFs are allowed in guard clauses. is_xxx self() to get the PID of the current process. Sending the current process\u0026rsquo;s PID in the message is a convention that allows the receiver to know whom to reply to. Storing state of the function on the stack i.e on function parameters Modules have functions and attributes. attrs start with - -define(MACRO, val) would be used as ?MACRO receive..after..end to specify timeouts. Sounds similar to Go timer ticks spawn(Module, Fn, Arg) to spawn a new process that executes Fn. Args is a list of args that will be passed to Fn. Arg will always be a list. with a full mail box, the messages are tried in order. If a msg doesn\u0026rsquo;t match any of the patterns, then it is put on the save q and the next message is tried. If it matches, then the messages from the save q is put back on top of the mailbox. This is called selective receive. nest another receive in a fn within the after 0 block to do selective receive. messages that do not match a pattern are never lost. They\u0026rsquo;re always around. The downside is this could lead to mailbox pollution if the proc doesn\u0026rsquo;t have the patterns to receive the msg. In a defensive programming style, the Unexpected match is used as a catchall to prevent mailbox pollution. link(pid) links the current process with pid. unlink(pid). When one of the linked processes crashes, the other crashes (exits?) too. Links are bidirectional. Since link(spawn(..)) is a multistep op, there could be a case where the process dies before it is spawned. This can cause undefined behaviour. In order to avoid this spawn_link(..) can be used which works in an atomic way. exit(blah) gets propagated as a special message (\u0026ldquo;signal\u0026rdquo;) which cannot be caught using normal receive. It can be caught if process_flag(trap_exit, true) is set. Then {'EXIT', Pid, msg} can be caught. monitors are like links, but they\u0026rsquo;re unidirectional and can be stacked. erlang:monitor(process, spawn(...)). spawn_monitor is the atomic alternative. exit(Pid, reason) to kill another process. Each process can be registered against a name which can then be used for sending messages instead of Pid. register(name, pid). unregister(name). registered() / regs() to get info about the registered processes. A process can have only name and a name can only be registered once. whereis(registered_name) to get the pid associated with it. This can be used in patterns to ensure that the reply is indeed from the process we expect. Another pattern is to send a ref to the proc and expect it back in the reply. This prevents us from expecting a reply from a specific Pid and shields us against scenarios where the process gets restarted. make_ref(). Refs are used when we expect a message from a certain source. In cases, we expect a message (ex: notification) and don\u0026rsquo;t care about the source, then the ref can be omitted. If a monitor is set for a process that\u0026rsquo;s already down using erlang:monitor(process, PID) then we receive a {'DOWN', MonitorRef, process, PID, \u0026lt;reason\u0026gt;} message as the reply. A pattern that I\u0026rsquo;ve seen is that the proc file contains method that can be used for invoking all the messages that the proc expects. Calls that require reply follow the pattern {Pid, Ref, Msg} \u0026ldquo;Walking on water and developing software from a specification are easy if both are frozen.\u0026rdquo;\n","permalink":"//localhost:1313/posts/11-notes-erlang/","summary":"\u003cp\u003eCollection of unstructed notes that I accumulated while learning Erlang. This is a \u003cem\u003ework-in-progress\u003c/em\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eerlang programs are composed on communicating processing. Like modeling objects, processes should be modeled to fit the problem. This is called modeling concurrency.\u003c/li\u003e\n\u003cli\u003econcurrency is about structure. parallelism is about execution.\u003c/li\u003e\n\u003cli\u003eeach expression must end with a \u003ccode\u003e.\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003evariables in erlang can only be bound once. Variables start with UPPERCASE letters. atoms begin with lowercase letters.\u003c/li\u003e\n\u003cli\u003eprocesses evaluate fns that are defined in modules. Modules are files with \u003ccode\u003e.erl\u003c/code\u003e extension.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epid ! {client_pid, message}\u003c/code\u003e for sending message to pid.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ec(module_name)\u003c/code\u003e to compile the module. \u003ccode\u003ef()\u003c/code\u003e  to forget the bindings.\u003c/li\u003e\n\u003cli\u003efile server and client. client provides the abstraction and hides the details of communication with the actual process. This gives use the flexibility to change the underlying implementation without changing the interface exposed through the client. \u003cem\u003eIt\u0026rsquo;s refreshing how Joe Armstrong doesn\u0026rsquo;t talk down to the programmer in this book. The innards and complicated lingua franca is exposed for everyone to see.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eerlang can handle arbitrary precision numbers. Like really big numbers.\u003c/li\u003e\n\u003cli\u003e= is more of a pattern matching operator rather than assignment operator.\u003c/li\u003e\n\u003cli\u003eatoms are similar to symbolic constants or enums. atoms are global. atoms can be quoted and can have spaces in them.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e{item1, blah}\u003c/code\u003e represents a tuple of fixed size. Since tuples don\u0026rsquo;t have type, it\u0026rsquo;s a convention to add an atom as the first element indicating the type. \u003ccode\u003e{point, 0, 1}\u003c/code\u003e. \u003ccode\u003e{point, C, C} = {point, 25, 25}\u003c/code\u003e works!\u003c/li\u003e\n\u003cli\u003estrings are represented as a list of integers with each int representing an unicode point.\u003c/li\u003e\n\u003cli\u003efullstop separates expression. comma separates related subordinate clauses. semicolon separates clauses.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003efun(arg) -\u0026gt; body end.\u003c/code\u003e to define anonymous fns.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e[f(X) || X \u0026lt;- L]\u003c/code\u003e list comprehension. \u003ccode\u003eX \u0026lt;- L\u003c/code\u003e follows the pattern logic for =.\u003c/li\u003e\n\u003cli\u003eNamed function F/n should be passed in as \u003ccode\u003efun F/n\u003c/code\u003e when used as an argument\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-record(Name { field1 = DefaultValue1, .... fieldN }\u003c/code\u003e.  \u003ccode\u003eundefined\u003c/code\u003e is the default value. \u003ccode\u003e.hrl\u003c/code\u003e files are like C header files where common definitions can be kept. \u003ccode\u003e#Name{key1=val1...}.\u003c/code\u003e to instantiate the struct. \u003ccode\u003err(\u0026quot;record_file.hrl\u0026quot;)\u003c/code\u003e to bring it into the erl.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eX#Name{field1=NewValue}\u003c/code\u003e to create a new record from an existing record X with a field value changed.\u003c/li\u003e\n\u003cli\u003eMaps are weird. \u003ccode\u003e#{ key Op val, key2 Op val2}\u003c/code\u003e. \u003ccode\u003e:=\u003c/code\u003e for updating an existing key. =\u0026gt; for adding a new key. The update follows the same pattern as records.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e;\u003c/code\u003e is OR and \u003ccode\u003e,\u003c/code\u003e is AND for guard sequences. maps : get/find etc for acessing map values.\u003c/li\u003e\n\u003cli\u003eclauses of a fn need to be separated by a SEMICOLON instead of a PERIOD!\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e\u0026lt;\u0026lt;\u0026quot;binaries\u0026quot;\u0026gt;\u0026gt;\u003c/code\u003e. Binary values must be in the range 0-255. Any other value would wrap around and be mapped to a value in the 0-255 range. term_to_binary and binary_to_term.\u003c/li\u003e\n\u003cli\u003ea type is binary when it\u0026rsquo;s size is divisible by 8, otherwise it\u0026rsquo;s a bitstring.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e\u0026lt;\u0026lt;R:5, B:6, G:5\u0026gt;\u0026gt;\u003c/code\u003e to pack elements into a binary while specifying the bit size. The same pattern HAS to be used when unpacking.\u003c/li\u003e\n\u003cli\u003etype test BIFs are allowed in guard clauses. \u003ccode\u003eis_xxx\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eself()\u003c/code\u003e to get the PID of the current process. Sending the current process\u0026rsquo;s PID in the message is a convention that allows the receiver to know whom to reply to.\u003c/li\u003e\n\u003cli\u003eStoring state of the function on the stack i.e on function parameters\u003c/li\u003e\n\u003cli\u003eModules have functions and attributes. attrs start with \u003ccode\u003e-\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e-define(MACRO, val)\u003c/code\u003e would be used as \u003ccode\u003e?MACRO\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ereceive..after..end\u003c/code\u003e to specify timeouts. Sounds similar to Go timer ticks\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003espawn(Module, Fn, Arg)\u003c/code\u003e to spawn a new process that executes Fn. Args is a list of args that will be passed to Fn. Arg will \u003cstrong\u003ealways\u003c/strong\u003e be a list.\u003c/li\u003e\n\u003cli\u003ewith a full mail box, the messages are tried in order. If a msg doesn\u0026rsquo;t match any of the patterns, then it is put on the save q and the next message is tried. If it matches, then the messages from the save q is put back on top of the mailbox. This is called selective receive. nest another receive in a fn within the \u003ccode\u003eafter 0\u003c/code\u003e block to do selective receive.\u003c/li\u003e\n\u003cli\u003emessages that do not match a pattern are never lost. They\u0026rsquo;re always around. The downside is this could lead to mailbox pollution if the proc doesn\u0026rsquo;t have the patterns to receive the msg.\u003c/li\u003e\n\u003cli\u003eIn a defensive programming style, the \u003ccode\u003eUnexpected\u003c/code\u003e match is used as a catchall to prevent mailbox pollution.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003elink(pid)\u003c/code\u003e links the current process with pid. \u003ccode\u003eunlink(pid)\u003c/code\u003e. When one of the linked processes crashes, the other crashes (exits?) too. Links are bidirectional.\u003c/li\u003e\n\u003cli\u003eSince link(spawn(..)) is a multistep op, there could be a case where the process dies before it is spawned. This can cause undefined behaviour. In order to avoid this spawn_link(..) can be used which works in an atomic way.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eexit(blah)\u003c/code\u003e gets propagated as a special message (\u0026ldquo;signal\u0026rdquo;) which cannot be caught using normal receive. It can be caught if \u003ccode\u003eprocess_flag(trap_exit, true)\u003c/code\u003e is set. Then \u003ccode\u003e{'EXIT', Pid, msg}\u003c/code\u003e can be caught.\u003c/li\u003e\n\u003cli\u003emonitors are like links, but they\u0026rsquo;re unidirectional and can be stacked. \u003ccode\u003eerlang:monitor(process, spawn(...))\u003c/code\u003e. spawn_monitor is the atomic alternative.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eexit(Pid, reason)\u003c/code\u003e to kill another process.\u003c/li\u003e\n\u003cli\u003eEach process can be registered against a name which can then be used for sending messages instead of Pid. \u003ccode\u003eregister(name, pid)\u003c/code\u003e. \u003ccode\u003eunregister(name)\u003c/code\u003e. \u003ccode\u003eregistered() / regs()\u003c/code\u003e to get info about the registered processes. A process can have only name and a name can only be registered once. \u003ccode\u003ewhereis(registered_name)\u003c/code\u003e to get the pid associated with it. This can be used in patterns to ensure that the reply is indeed from the process we expect.\u003c/li\u003e\n\u003cli\u003eAnother pattern is to send a ref to the proc and expect it back in the reply. This prevents us from expecting a reply from a specific Pid and shields us against scenarios where the process gets restarted. \u003ccode\u003emake_ref()\u003c/code\u003e. Refs are used when we expect a message from a certain source. In cases, we expect a message (ex: notification) and don\u0026rsquo;t care about the source, then the ref can be omitted.\u003c/li\u003e\n\u003cli\u003eIf a monitor is set for a process that\u0026rsquo;s already down using \u003ccode\u003eerlang:monitor(process, PID)\u003c/code\u003e then we receive a \u003ccode\u003e{'DOWN', MonitorRef, process, PID, \u0026lt;reason\u0026gt;}\u003c/code\u003e message as the reply.\u003c/li\u003e\n\u003cli\u003eA pattern that I\u0026rsquo;ve seen is that the proc file contains method that can be used for invoking all the messages that the proc expects.\u003c/li\u003e\n\u003cli\u003eCalls that require reply follow the pattern \u003ccode\u003e{Pid, Ref, Msg}\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;Walking on water and developing software from a specification are easy if both are frozen.\u0026rdquo;\u003c/p\u003e","title":"Notes On Erlang"},{"content":"When I\u0026rsquo;m working, I usually have at-least 4 windows open including Slack, Chrome, iTerm and a few other stuff. Lately, I\u0026rsquo;ve noticed that switching between them via cmd+tab is turning out to be a waste of time, since the order of the apps keep changing based on how recently they were used.\nI wanted a smoother transition between my umpteen apps without getting stuck in cmd+tab hell.\nI needed static key-bindings!\nThat\u0026rsquo;s when I found Hammerspoon. Hammerspoon is a neat little tool that allows you to write little automations for a wide range of stuff on Mac. It\u0026rsquo;s completely configurable and supports Lua as the extension langague, which is even cooler. And to top it all off, the documentation is thorough and on-point.\nI was able to hack together a tiny script to serve my needs.\ninit.lua for Hammerspoon:\nfunction open_app(name) return function() hs.application.launchOrFocus(name) end end hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;1\u0026#34;, open_app(\u0026#34;iTerm\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;2\u0026#34;, open_app(\u0026#34;Visual Studio Code - Insiders\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;3\u0026#34;, open_app(\u0026#34;Visual Studio Code\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;4\u0026#34;, open_app(\u0026#34;Slack\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;5\u0026#34;, open_app(\u0026#34;Preview\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;7\u0026#34;, open_app(\u0026#34;MacVim\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;8\u0026#34;, open_app(\u0026#34;Google Chrome\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;9\u0026#34;, open_app(\u0026#34;Firefox\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;\u0026#39;\u0026#34;, open_app(\u0026#34;Spotify\u0026#34;)) hs.hotkey.bind({\u0026#34;cmd\u0026#34;}, \u0026#34;;\u0026#34;, open_app(\u0026#34;Postman\u0026#34;)) This might seem trivial and even borderline insane, but the payout has been immense with a smoother workflow with fewer impediments.\nThis was inspired by jasonrudolph/keyboard who has done some pretty cool stuff with Hammerspoon.\n","permalink":"//localhost:1313/posts/10-multitasking-hammerspoon/","summary":"\u003cp\u003eWhen I\u0026rsquo;m working, I usually have at-least 4 windows open including Slack, Chrome, iTerm and a few other stuff. Lately, I\u0026rsquo;ve noticed that switching between them via \u003ccode\u003ecmd+tab\u003c/code\u003e is turning out to be a waste of time, since the order of the apps keep changing based on how recently they were used.\u003c/p\u003e\n\u003cp\u003eI wanted a smoother transition between my umpteen apps without getting stuck in \u003ccode\u003ecmd+tab\u003c/code\u003e hell.\u003c/p\u003e\n\u003cp\u003eI needed static key-bindings!\u003c/p\u003e","title":"Faster Multitasking with Hammerspoon"},{"content":"What do you think the output of the following code would be?\npackage main import \u0026#34;fmt\u0026#34; func main() { input := \u0026#34;hello\u0026#34; TestDefer(\u0026amp;input) } func TestDefer(input *string) { defer fmt.Println(*input) *input = \u0026#34;world\u0026#34; fmt.Println(*input) } Given how defer-ed functions are executed just before the parent function exits, I expected the output to be\nworld world But, on execution it actually prints\nworld hello This is because the arguments are evaluated when the defer is encountered, and not when the deferred function is actually called. Effective Go even has a line specifically about this behavior (which I discovered later).\nThis makes sense if you think of defer as a function that gets executed each time it is encountered. The result of the execution is that it sets up the deferred function to be executed right before the parent function exits.\nThis whole journey started with a piece of code I was debugging where updates to a piece of data were not being saved to the underlying storage layer.\nfunc doSomething(txn, ....) { ... defer store.Save(txn) ... // modify txn here ... return } Wrapping it in an anonymous function helped alleviate my problem\nfunc doSomething(txn, ....) { ... // fugly, but works! defer func() { store.Save(txn) }() ... // modify txn here ... return } That was an evening well spent. I do find that I enjoy these sort of bug adventures which end up correcting some flawed mental model I previously had about a system. They\u0026rsquo;re the most fulfilling.\n","permalink":"//localhost:1313/posts/09-sneaky-defers-in-go/","summary":"\u003cp\u003eWhat do you think the output of the following code would be?\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003epackage\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f92672\"\u003eimport\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;fmt\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003emain\u003c/span\u003e() {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003einput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e:=\u003c/span\u003e \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;hello\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eTestDefer\u003c/span\u003e(\u003cspan style=\"color:#f92672\"\u003e\u0026amp;\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einput\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eTestDefer\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003einput\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e) {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#66d9ef\"\u003edefer\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einput\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einput\u003c/span\u003e = \u003cspan style=\"color:#e6db74\"\u003e\u0026#34;world\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003efmt\u003c/span\u003e.\u003cspan style=\"color:#a6e22e\"\u003ePrintln\u003c/span\u003e(\u003cspan style=\"color:#f92672\"\u003e*\u003c/span\u003e\u003cspan style=\"color:#a6e22e\"\u003einput\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eGiven how \u003ccode\u003edefer\u003c/code\u003e-ed functions are executed just before the parent function exits, I expected the output to be\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-nil\" data-lang=\"nil\"\u003eworld\nworld\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eBut, on execution it actually prints\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode class=\"language-nil\" data-lang=\"nil\"\u003eworld\nhello\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eThis is because the arguments are evaluated when the defer is encountered, and not when the deferred function is actually called. Effective Go even has a line \u003ca href=\"https://golang.org/doc/effective_go#defer\"\u003especifically about this behavior\u003c/a\u003e (which I discovered later).\u003c/p\u003e","title":"Sneaky Defers In Go"},{"content":"A while back, I came across this article by Josh Branchaud where he talked about TIL posts and learning in public (among many other things). This really appealed to me. Over the years, I had accumulated immense amount of information from posts and articles that people had put out and the whole idea of paying it forward by putting out things that I\u0026rsquo;d learned along the way sounded interesting.\nThe primary impediment I faced here was the amount of time it took to create a well-crafted post. Most of the longer pieces here were summaries of months of effort squeezed into a single page post with pictures. Those were few and far apart.\nI needed another form to publish the little snippets of information that I usually encountered on a day-to-day basis. This could be trivial things like an obscure command line flag or an interesting HN comment. As opposed to posts, which were original pieces, these would mostly be derivate content with comments.\nAlso, huge shout-out to Simon Willison who\u0026rsquo;s been blogging since 2002. When I got into a funk, I checked out his early posts to see how he had started. They were mostly a few lines describing what he\u0026rsquo;d done, or about stuff he had found on the internet. That gave me a huge boost in putting my own stuff out there.\nP.S: You might see new posts with older dates. This is just me moving my notes from a private git-repo into the blog.\n","permalink":"//localhost:1313/posts/08-writers-block/","summary":"\u003cp\u003eA while back, I came across \u003ca href=\"https://dev.to/jbranchaud/how-i-built-a-learning-machine-45k9\"\u003ethis article\u003c/a\u003e by Josh Branchaud where he talked about TIL posts and learning in public (among many other things). This really appealed to me. Over the years, I had accumulated immense amount of information from posts and articles that people had put out and the whole idea of paying it forward by putting out things that I\u0026rsquo;d learned along the way sounded interesting.\u003c/p\u003e\n\u003cp\u003eThe primary impediment I faced here was the amount of time it took to create a well-crafted post. Most of the longer pieces here were summaries of months of effort squeezed into a single page post with pictures. Those were few and far apart.\u003c/p\u003e","title":"Writer's Block"},{"content":"I realized that I hadn\u0026rsquo;t touched my blog in over a year. A lot had happened since then, but I hadn\u0026rsquo;t taken the time to note anything down. Looking through my old posts, I remember how good it felt to actually sit and write things down.\nI thought I\u0026rsquo;d change the theme and tweak things a bit just for the sake of it. Lo and behold, I ended up spending an entire day trying out themes and tweaking knobs and controls to see how things looked. Sometimes, I feel like I waste a lot of time on preliminary aspects of a task.\nHopefully, I should be able to hammer out something tomorrow.\n","permalink":"//localhost:1313/posts/07-back-after-hiatus/","summary":"\u003cp\u003eI realized that I hadn\u0026rsquo;t touched my blog in over a year. A lot had happened since then, but I hadn\u0026rsquo;t taken the time to note anything down. Looking through my old posts, I remember how good it felt to actually sit and write things down.\u003c/p\u003e\n\u003cp\u003eI thought I\u0026rsquo;d change the theme and tweak things a bit just for the sake of it. Lo and behold, I ended up spending an entire day trying out themes and tweaking knobs and controls to see how things looked. Sometimes, I feel like I waste a lot of time on preliminary aspects of a task.\u003c/p\u003e","title":"Back After A Hiatus"},{"content":"Recently, I started randomly going through the Go standard library, mostly to satiate my curiosity and to find out what goes on behind the curtains. While checking out the testing package, I found this interesting little snippet of code in src/testing/testing.go:\n// TB is the interface common to T and B. type TB interface { Cleanup(func()) Error(args ...interface{}) Errorf(format string, args ...interface{}) Fail() FailNow() Failed() bool Fatal(args ...interface{}) Fatalf(format string, args ...interface{}) Helper() Log(args ...interface{}) Logf(format string, args ...interface{}) Name() string Skip(args ...interface{}) SkipNow() Skipf(format string, args ...interface{}) Skipped() bool TempDir() string // A private method to prevent users implementing the // interface and so future additions to it will not // violate Go 1 compatibility. private() } This seems pretty evident once you see it. It makes sense for the Go standard library where the private function enables them to circumvent the compatibility promise by ensuring that no one would be able to use this interface outside of the standard library because of the private function. This gives them the flexibility to add functionality later without breaking anything.\nI wonder if there\u0026rsquo;s a general lib out in the wild which uses this technique.\n","permalink":"//localhost:1313/posts/06-unimplementable-interfaces-go/","summary":"\u003cp\u003eRecently, I started randomly going through the Go standard library, mostly to satiate my curiosity and to find out what goes on behind the curtains. While checking out the testing package, I found this interesting little snippet of code in \u003ccode\u003esrc/testing/testing.go\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-go\" data-lang=\"go\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#75715e\"\u003e// TB is the interface common to T and B.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#66d9ef\"\u003etype\u003c/span\u003e \u003cspan style=\"color:#a6e22e\"\u003eTB\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eCleanup\u003c/span\u003e(\u003cspan style=\"color:#66d9ef\"\u003efunc\u003c/span\u003e())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eError\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eErrorf\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eformat\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eFail\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eFailNow\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eFailed\u003c/span\u003e() \u003cspan style=\"color:#66d9ef\"\u003ebool\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eFatal\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eFatalf\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eformat\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eHelper\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eLog\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eLogf\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eformat\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eName\u003c/span\u003e() \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eSkip\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eSkipNow\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eSkipf\u003c/span\u003e(\u003cspan style=\"color:#a6e22e\"\u003eformat\u003c/span\u003e \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e, \u003cspan style=\"color:#a6e22e\"\u003eargs\u003c/span\u003e \u003cspan style=\"color:#f92672\"\u003e...\u003c/span\u003e\u003cspan style=\"color:#66d9ef\"\u003einterface\u003c/span\u003e{})\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eSkipped\u003c/span\u003e() \u003cspan style=\"color:#66d9ef\"\u003ebool\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eTempDir\u003c/span\u003e() \u003cspan style=\"color:#66d9ef\"\u003estring\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#75715e\"\u003e// A private method to prevent users implementing the\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#75715e\"\u003e// interface and so future additions to it will not\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#75715e\"\u003e// violate Go 1 compatibility.\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\t\u003cspan style=\"color:#a6e22e\"\u003eprivate\u003c/span\u003e()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis seems pretty evident once you see it. It makes sense for the Go standard library where the private function enables them to circumvent the compatibility promise by ensuring that no one would be able to use this interface outside of the standard library because of the private function. This gives them the flexibility to add functionality later without breaking anything.\u003c/p\u003e","title":"Un-implementable Interfaces In Go"},{"content":"Background Work always manages to throw interesting problems my way and this one was particularly interesting. Our telephone server infrastructure and the associated cloud services were spread across two AWS regions - Singapore \u0026amp; Mumbai. This was primarily done to comply with Indian Data Protection Laws which mandated that customer data associated with some critical areas of business must stay within the country. We had run these two regions as independent entities, with code changes being deployed uniformly across them.\nOwing to some changes we had done as part of another unification project, we managed to make the physical servers agnostic of the AWS region. It allowed us to move away from statically assigning servers to a region, and to shift capacity between regions based on demand. As a byproduct of this unification project, we had to reconcile and merge the telephone server data that was currently spread across two databases which were hosted in these two regions.\nThe Problem We had two MySQL databases housing telephone server related information in each of our two regions. The goal was to unify the view of data so that it would be the same everywhere. Essentially, the result of running a query on this data should yield the same result regardless of the region it was executed in. We had about 9 tables whose data had to be merged.\nThere were two impediments that faced us:\nPrimary Key(PK) conflicts: PKs were reused across regions, since they were agnostic of each other, which would cause problems if we went for a blind merge. Foreign Key(FK) dependency: This is primarily a side effect on the above. Any change in PKs should take the FK relationships into account so that data consistency is maintained at the end of the operation. The Solution Our databases were slightly asymmetrical such that one region had significantly more data than the other. Adding an offset to the PKs in the smaller DB would ensure that the PKs are continuous and conflict free between the regions. Once the PKs were fixed, we could take a dump and merge the data.\nTo keep the foreign key relationships intact, the changes would have to be propagated to all the tables that referenced these PK columns. The reference relationship can be obtained using the INFORMATION_SCHEMA.KEY_COLUMN_USAGE table. A simple query like the one detailed in this SO answer would get us all the tables referring to a particular column of a chosen table. When you\u0026rsquo;re working with multiple tables with multiple relationships, it\u0026rsquo;s always best to visualize this information to make tracking a little easier. The edges on the graph below denote the column of the referring table which refers to the PK of the referred table.\nWe prepared the list of queries and scripts to be executed beforehand to minimize downtime and to prevent manual errors. MySQL supports Prepared Statements which is sort of like a DSL that allows us to create (or \u0026ldquo;prepare\u0026rdquo;) SQL statements and then execute them. It has basic support for variables, which allows us to write generic SQL queries that can be applied to a lot of tables through the use of variables. This enabled us to cut the canned query size to a large extent.\nThe Execution Because of the nature of our system, we could never completely freeze access to the DBs. So we started with the activity during a lean period, when traffic was negligible to minimize outward impacts.\nWe started by taking a backup of the DBs in both the regions, just to be extra safe. There are a few system level variables that MySQL maintains which dictates the behaviour of the database engine. One of them is foreign_key_checks which indicates whether foreign key constraints would be respected or not. This constraint flag was disabled during the migration, since there was no way to alter the PK without violating the FK relationships. The canned statements were then executed on the smaller DB to fix the PKs and FKs. Once the PK changes were back-propagated, foreign_key_checks were enabled again. Once the changes were made and canned queries were executed in the smaller DB, it was merely a matter of taking a mysqldump from each region and applying it in the other region.\nLessons Learned ALWAYS take backups. The more the better. I\u0026rsquo;ve seen multiple downtimes but messing with production databases and unifying data at this scale remains the single most scariest thing I\u0026rsquo;ve done to pubDate. So, it\u0026rsquo;s always good to err on the side of caution, even if it\u0026rsquo;s a slower and longer path. Freeze access to your DBs during data migration: We found that one of the update queries from an automated script had gotten through during the migration phase which resulted in the FK relations getting screwed up. Thankfully, MySQL prevents any updates to a tables once it detects a violation of FK constraints. This allowed us to zero in on the problem and fix it. Use prepared statements and canned SQL statements for execution to minimize human error. Supposedly, the whole process would\u0026rsquo;ve been a lot easier if we used UUIDs instead of auto incremented ints for our PKs. There\u0026rsquo;s a wealth of opinions on the web arguing for and against this approach.\nPing me your thoughts and comments.\n","permalink":"//localhost:1313/posts/05-tale-of-two-dbs/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eWork always manages to throw interesting problems my way and this one was particularly interesting. Our telephone server infrastructure and the associated cloud services were spread across two AWS regions - Singapore \u0026amp; Mumbai. This was primarily done to comply with Indian Data Protection Laws which mandated that customer data associated with some critical areas of business must stay within the country. We had run these two regions as independent entities, with code changes being deployed uniformly across them.\u003c/p\u003e","title":"A Tale Of Two DBs"},{"content":"This book had been on my TO-READ list for a long time. It came up again while I was perusing Dan Luu\u0026rsquo;s Programming book list. I\u0026rsquo;ve always wanted to look behind the curtains and see how the magic worked, so I finally bought it.\nI used bootlin to read through Linux 5.7.2 source. They provide a really good search system and linked definitions. The book describes kernel version 2.6. You might want to keep this site open to see how things have changed since then.\nProcess \u0026amp; Threads A process begins it life with fork()\nLifecycle: fork() [Create a copy of the current running process] -\u0026gt; exec() [Load a binary into memory] -\u0026gt; exit()\nMetadata about each process is stored in a task_struct. Info about all processes are maintained in a linked-list called the tasklist. They\u0026rsquo;re often referred to as process descriptors.\nthread_info struct is present at the bottom of the stack (for stacks that grow down). This allows for a lot of neat optimizations whereby the thread_info of the current process can be computed and found pretty quickly(review).\nfork() is implemented through Copy-On-Write (COW) pages. Resources are duplicated only when they are modified. The gain comes through not duplicating the address space!\nThreads in linux are no different from processes. Each thread has it\u0026rsquo;s own task_struct and is scheduled like any other task. Certain params in the task_struct have common values to indicate that resources are shared. This is different from Windows where threads are seen as lightweight processes, where the kernel has explicit support for dealing with threads.\nKernel threads are a special class of threads that run only in kernel space. Forked from kthreadd for performing special ops like flush, ksoftirqd.\nScheduling O(1) scheduler, followed by the Completely Fair Scheduler\nSticking to conventional ideas of an absolute time slice ensures constant switching rate but variable fairness and can lead to a slew of problems. CFS does away with this by ditching timeslices and allocating a portion of the processor to each process. This results in variable switching rate but constant fairness.\nCFS works by assuming that there is an ideal processor that is capable of multitasking. If we have n processes, each would run in parallel, consuming 1/n of the processor. Reality deviates from this ideal dream in the fact that perfect multitasking is not possible, and that there is an overhead involved in switching processes. Nevertheless, CFS is designed with the idea of giving a portion of the processor to each running process. This portion assigned is a function of the total number of processes waiting to be run. Nice values are used here to weight the processor portion that each process receives - a lower niceness value would result in a relatively higher portion of the processor. Thus when we take an infinitely small time window, each process would\u0026rsquo;ve been scheduled for a time slice proportional to their processor portion.\nThis infinitely small window is usually approximated to a duration called targeted latency. Smaller value results in higher interactivity since it approximates the ideal case, but it results in lower throughput because of switching overhead. targeted latency is floored at a value called minimum granularity by the kernel.\nAll the scheduling info is carried in sched_entity which is embedded in each task_struct.\nThe most interesting thing here is the vruntime, the virtual run time, which is what the scheduler uses to pick the next process. There is a concept of physical time and virtual time. Physical run time is the actual time that the process ran and virtual run time is normalized physical time computed using the number of runnable processes and the niceness value of the process. Approximately, it is computed as physcial_time * (NICE_0_LOAD / proc_load) where NICE_0_LOAD represents the weight of a process who\u0026rsquo;s niceness value is 0 and proc_load represents the weight of the process calculated using its niceness value. Thus for processes with lower niceness value (higher priority), the virtual time would be less than physical time and vice versa. Thus they\u0026rsquo;d get a bigger portion of the processor in turn. This SO answer goes into some more depth.\nCFS maintains runnable procs in a red-black tree where the key is the vruntime. It continuously picks and schedules the process with the lowest vruntime. It does a neat optimization where it caches the left-most node during insertion / deletion of each new node.\nWhen a task goes to sleep, it marks itself as sleeping, puts itself on a wait Q, removes itself from the red-black tree of runnables and calls schedule() to select the new process to execute. To wake up the task, it is marked as runnable, removed from the wait Q, and put back in the runnable tree.\nSystem Calls System calls provide an interface between the applications in user space and the kernel. They provide a mechanism through which applications can safely interact with the underlying hardware, create new processes, communicate with each other, as well as the capability to request other operating system resources. Provide mechanism, not policy. The kernel system calls provide a specific fn. The manner in which it is used does not matter to the kernel.\nUser space applications cannot directly invoke a kernel function. The whole communications happens through register values and interrupts. Each syscall has a particular value associated with it. This value is loaded into the eax register and then an interrupt is invoked int 0x80 which invokes the interrupt handler which hands over control to the kernel, which then executes the appropriate system call on behalf of the user space application.\nMost of the system calls are defined with the funky SYSCALL_DEFINE macro. This answer explains the curious asmlinkage that gets prefixed to these functions. Syscall bar is referred to as syscall_bar within the kernel.\nKernel Data Structures The ubiquitous linked list implementation is a circular doubly linked list\u0026hellip; with some quirks. Unlike usual linked lists, the data is not embedded within the linked list struct but rather the linked list struct struct list_head is embedded within the data struct. The kernel uses some C macro magic with container_of to get a pointer to the embedding struct from the list_head pointer. This post demystifies the magic behind the macro.\nIn addition the kernel code also contains implementations for a queue (with the usual ops) and a map. The map is implemented as a balanced binary search tree with a rather confusing name - idr. It provides mapping between UIDs to pointers.\nInterrupts \u0026amp; Interrupt Handlers Interrupts generated by H/W are handled by specific Interrupt Handlers or Interrupt Service Routines (ISR). Generally the ISR for a device is part of the device driver code in the kernel. ISR in the kernel are nothing but C functions that run in the interrupt context (atomic context). The work associated with handling an interrupt is divided into two parts -\nAcknowledging the H/W and performing operations that will enable the H/W will proceed further (stuff like copying all the received packets from a NIC\u0026rsquo;s buffer) - This is handled by the \u0026lsquo;Top Half\u0026rsquo;. Further work on the data associated with the kernel, which is not critical and can be performed at a future point in time - This is handled by the \u0026lsquo;Bottom Half\u0026rsquo;. An interrupt handler is registered for an IRQ line using request_irq() which takes in information about the IRQ number, handler fn, flags pertaining to the nature of the interrupt and handler, and some extra stuff. The registration happens when the driver is loaded. Similarly, when the driver is unloaded the handler needs to be freed using free_irq().\nInterrupt handlers in linux need to be reentrant i.e the handler will not be invoked concurrently. When an interrupt is being service, the interrupt line is disabled (masked) which prevents further interrupts from coming on that line. Thus it is guaranteed that the ISR won\u0026rsquo;t be invoked in parallel.\nInterrupt lines may be shared among multiple handlers. For a line to be shared, each handler on that line must be registered as a shared handler. The handler returns a value denoting whether the interrupt was handled or not. When an interrupt is received on a shared line, the kernel invokes each of the handlers one by one. It uses the return value to ascertain whether the interrupt was handled.\nInterrupt handlers run in the interrupt context. Since it is not backed by a process, ISR are not allowed to sleep (who will wake it up and how?), which restricts the activities that can be done from ISR. Earlier ISR was forced to use the stack of the process it interrupted. Now, there is an interrupt stack associated with the kernel which is of size equivalent to one page which the ISR can use.\ncat /proc/interrupts shows the interrupt line, the number of interrupts received by each CPU, the interrupt controller, the interrupt type, and the device.\nBottom halves may be implemented using softirqs, tasklets, or work queues.\nSynchronization Locks are implemented using atomic test and set insturctions that are provided by the underlying architecture. Atomic operations using atomic_t. There\u0026rsquo;s a lot to talk about here. In the book, which is based on linux kernel 2.6, atomic_t is implemented as a volatile int inside a struct. The struct was chosen so that there would be no way to cast it into another valid form. The choice of volatile does not seem to have survived the test of time, with more recent variants moving away from it altogether. This document goes through the structure and behavior of the latest version of atomic_t, in addition, it also sheds light on why volatile should not be used. Most of these arise because of the reliance on volatile to enforce a memory barrier while it actually does not. This SO answer illustrates cases where volatile can be justifiably used to prevent the compiler from optimising away checks and conditions that rely on MMIO.\natomic_t provides atomic operations to manipulate integers and bits. This is useful in cases where the critical region does not perform any operation more complicated than that. Locks are used when the critical regions are more complex, where multiple operations need to be performed while still ensuring atomicity.\nSpin lock are a form of locking provided by the kernel, where the thread busy waits (spins) until the lock is acquired. This might seem inefficient in comparison to scenarios where the threads are put to sleep if the lock is not available. In cases where the locks are held for a short duration of time (or in code paths where you cannot sleep), spinlocks are efficient as it foregoes the overhead of scheduling involved with sleeping the thread and waking it up. An interesting fact is that the locks compile away in uniprocessor machines to markers that disable and enable kernel pre-emption. Interrupt handlers can use spin-locks, provided local interrupts to the current processor are disabled (this ensures that we do not get stuck with a double acquire deadlock). Lock data not code. Reader/writer variant of the spin lock is also provided by the kernel. A RW spin lock always favors readers. A sufficient number of readers could cause the starvation of the writer!\nSemaphores in linux are sleeping locks. Can only be used in process context since it is not possible to sleep in an interrupt context (as they won\u0026rsquo;t be rescheduled). Variants : binary semaphores (mutex) and counting semaphores. In addition, there\u0026rsquo;s a rwsemaphore and a mutex(as a separate struct with a simpler interface).\nThere\u0026rsquo;s another curious thing called the completion variable provided by the kernel. This is useful is scenarios where one process is waiting for a singal from the other indicating completion. A semaphore can be used here, but completion variable provides a simpler interface.\nIn addition to all of this there\u0026rsquo;s the Big Kernel Lock (BKL) that was added to ease the SMP transition. It\u0026rsquo;s a recursive, global spin lock that can be used in the process context. It\u0026rsquo;s interesting to note how different projects got started with coarse grained locking, and later moved to fine grained locking as the project matured and the need for concurrency grew. I wonder when Python will tide over the Global Interpreter Lock(GIL).\nJust when you thought you couldn\u0026rsquo;t need another locking mechanism, the kernel throws seqlocks in your face. This is sort of like the RW locks seen earlier, with the difference being that writers are preferred over readers. Each locked data is associated with a sequence counter (which is the thing that\u0026rsquo;s protected by the lock). During a write, the counter is incremented. A read operation checks the sequence counter prior to and after the read. A read succeeds(as in, a write did not happen in between), if the values match. Thus, writers are never blocked, and dirty reads would just cause the reader to retry the read until the counter conditions are satisfied.\nOrdering \u0026amp; Barriers The processor and compiler might reorder memory accesses (reads and write) in code for a variety of reasons which might break some implicit assumptions that the code relies on. Barriers can be used to enforce the ordering and to indicate to the compiler / processor to maintain the order of operations. mb()/rmb()/wmb() provide memory barrier / read memory barrier / write memory barrier which ensure that rw / reads / writes are not reordered across them i.e all corresponding ops before the barrier are guaranteed to complete before the ops after it.\nMemory Management Pages are treated as the smallest unit of memory management. Memory is divided in multiple zone, each zone with a particular characteristic (DMA, normal, high memory etc). Allocations will never cross zone boundaries. Pages returned to user-space are zeroed out to ensure security. kmalloc allocates memory that is physically contiguous while vmalloc allocates memory that is contiguous in the virtual address space.\nFile Managements VFS abstraction layers allows userland programs to be agnostic of the underlying fs. Main components: superblock, inode, dentry, file.\nBlock Devices Sectors, blocks, buffers, and buffer heads. The IO scheduler mergers and sorts requests on the block device to maximize \u0026ldquo;global\u0026rdquo; throughput. Anticipatory, deadline, completely fair queuing, and noop variant.\nTODO Interesting but not interesting enough for now Interrupt Handler Bottoms - softirq, tasklets Slab allocator ","permalink":"//localhost:1313/posts/04-notes-linux-dev/","summary":"\u003cp\u003eThis book had been on my TO-READ list for a long time. It came up again while I was perusing \u003ca href=\"https://danluu.com/programming-books/\"\u003eDan Luu\u0026rsquo;s Programming book list\u003c/a\u003e. I\u0026rsquo;ve always wanted to look behind the curtains and see how the magic worked, so I finally bought it.\u003c/p\u003e\n\u003cp\u003eI used \u003ca href=\"https://elixir.bootlin.com/linux/v5.7.2/C/ident/task_struct\"\u003ebootlin\u003c/a\u003e to read through Linux 5.7.2 source. They provide a really good search system and linked definitions. The book describes kernel version 2.6. You might want to keep this site open to see how things have changed since then.\u003c/p\u003e","title":"Notes from 'Linux Kernel Development'"},{"content":"If all you have is a hammer\u0026hellip; Organizational choices and system architecture sometimes forces you to use sub-optimal tools for a problem. In fact, this is part of the challenge that work throws at you - having to retrofit or abuse tools to get the job done.\nIf you always had the right set of tools, what fun would life be? This is one such problem.\nThe Problem We had an antiquated use case which allowed customers to create a deferred list of jobs. These jobs would then be processed based on API requests from the customer\u0026rsquo;s end. These lists would usually range from about 100 - 100000 jobs. We also provided a provision whereby the customer could trigger multiple requests in parallel to enable concurrent processing of these jobs. The original design dumped these jobs into MySQL, given that these jobs had to be persisted indefinitely until a trigger was detected.\nStepping back from the nitty-gritty details, you can see that this is in essence a concurrent queue modeled on MySQL. The original implementation was not optimized for our traffic and it suffered from race conditions. We were handling a level of traffic which had caused DB outages in the past, so we did not want to lean on MySQL too much.\nGiven the scale of the traffic, the criticality of the DB to serve our operations, and the sensitivity of this use-case to latency, it was decided that Aersopike would be used as the primary data store instead of MySQL. As I\u0026rsquo;d mentioned in my previous post, we use Aerospike A LOT - mostly because it\u0026rsquo;s blazing fast and scalable, but also because it\u0026rsquo;s free. We have a data sync mechanism that syncs data from Aerospike to MySQL once the records have been processed.\nData stores were never meant to be used as a job queue and it required some effort to get Aerospike to do the same.\nThe FCFS Way The straightforward way is to implement a First Come First Server (FCFS) system whereby each incoming request would find the first unprocessed job, reserve it, and then proceed with its processing.\nIn a concurrent environment, whenever there\u0026rsquo;s a two step process to reserve a job, there\u0026rsquo;s bound to be race conditions - two requests could come up on the same job, reserve them, and then proceed with the processing of the same job. Even if we were to look past the race condition, this approach would take O(N) time to service to request in the worst case, with N being the total number of jobs in the queue. Ideally, we\u0026rsquo;d prefer to have a single operation to reserve the job.\nThe ID Store To prevent each request traversing the entire length of the job queue we set up a job ID Store which contains the list of all unprocessed jobs.\nThis was implemented in Aerospike using the list aggregate type, which we used to store the list of unprocessed job IDs. List pop operation (provided by Aerospike) allowed us to get a Job ID while still ensuring isolation between requests. In addition, the jobs were indexed based on job IDs for faster access.\nThus each request would first pop off from the ID store and select the corresponding job from the jobs set. This has the dual benefit of avoiding race conditions by leaning on the storage engine to ensure isolation, and decreasing the worst case job assignment complexity to O(1).\nThe only downside here is the Aerospike record limit. Each record in Aerospike is like a row in a SQL DB and Aerospike has a (configurable) limit on the size of each record. Unlucky for me, this limit was set at 128KB in our system. If we assume each job ID to be 8B, then we can accommodate only 16000 IDs per record.\nCan we do better?\nThe Token Store Optimization We had to store the list of job IDs because they\u0026rsquo;re usually non-contiguous numeric identifiers. We can forego this list, if we assign sequential token IDs to each job. This indexed field provides an alternative way to refer to jobs within a set. The ID Store, which we\u0026rsquo;ll now call the Token Store, will contain the token ID of the next job to be processed. You can think of it as a pointer to the job queue. This will help us tide over the record size limitations.\nTo reserve a job, a request would get the current token ID in the Token Store, fetch the corresponding record from the job set and then increment the token value so that it points to the next unprocessed job.\nWhile this looks efficient, it brings back the inevitable race condition - two jobs could read the same value and reserve the same job. What we need is an atomic operation to deal with the token ID.\nAersopike provides the facility to define User Defined Functions(UDF) in Lua which allows us to define new functions that are guaranteed to be atomic by the storage engine. So, we defined a UDF to implement the read-increment-write operation which would read the token value, increment it, write the incremented value, and return the old value. Thus, each incoming request would invoke the read-increment-write UDF on Token Store to get the token ID, and would use this ID to get the corresponding job.\nWe\u0026rsquo;ve thus managed to stick to O(1) for job assignment while cutting down the space requirement of the list.\nDrawbacks The primary downside is that we have no way to ensure fault tolerance. If a request, which reserved a job, dies then we have no way to put that job back into the pool of reserved jobs. Thus the optimization might not be useful in the general context, but was acceptable for our specific use-case.\nAlternatives Using a SQL DB Aerospike is a NoSQL datastore and thus do not provide the rich set of operations made available by SQL. As outlined in this answer on Database Administrator, SQL databases like MySQL enables us to use a combination of Transactions and SELECT FOR UPDATE to achieve the same result, albeit with a slightly higher performance penalty.\nWe couldn\u0026rsquo;t use it for our use-case as our MySQL DB was far too precious to be put under heavy load from such a bursty workflow.\nUsing a Message Queue A simple persistent message queue like Beanstalkd would\u0026rsquo;ve been a perfect fit for this problem. Message Queues have the concept of tubes, which provides a high level way to group messages, which could be used for organizing jobs from different customers into different tubes. They also provide facilities like delays, whereby a job is put back into the queue if the reserved consumer has not responded withing a stipulated time frame, which would take care of the fault tolerance aspect.\nWe couldn\u0026rsquo;t use this solution because our services had some design decisions baked in, which made integrating a message queue into the flow a non-trivial exercise.\nLessons Learned Your problem does not exist in a vacuum. Your possible solutions would be constrained by the environment you operate in. Technical decisions, especially in the context of services, have long term repercussions that would influence the enhancements and modifications that could be carried out on it. Be realistic about the effort involved in implementing the perfect solution, in view of the time constraints - job Queues would\u0026rsquo;ve been perfect, but an optimized Aerospike setup was the next best option. Know when to stop. Optimization are an unending rabbit hole. Prefer clarity over cleverness wherever possible. This blog post is my explanation for future maintainers of my code as to how things reached the state they are in now. I did what had to be done. :P\nPing me your thoughts and comments.\nCheck out Aerospike and Beanstalkd, if you haven\u0026rsquo;t already!\nAll diagrams were created using Sketchviz.\n","permalink":"//localhost:1313/posts/03-wrong-tool/","summary":"\u003ch2 id=\"if-all-you-have-is-a-hammer-dot-dot-dot\"\u003eIf all you have is a hammer\u0026hellip;\u003c/h2\u003e\n\u003cp\u003eOrganizational choices and system architecture sometimes forces you to use sub-optimal tools for a problem. In fact, this is part of the challenge that work throws at you - having to retrofit or abuse tools to get the job done.\u003c/p\u003e\n\u003cp\u003eIf you always had the right set of tools, what fun would life be? This is one such problem.\u003c/p\u003e\n\u003ch2 id=\"the-problem\"\u003eThe Problem\u003c/h2\u003e\n\u003cp\u003eWe had an antiquated use case which allowed customers to create a deferred list of jobs. These jobs would then be processed based on API requests from the customer\u0026rsquo;s end. These lists would usually range from about 100 - 100000 jobs. We also provided a provision whereby the customer could trigger multiple requests in parallel to enable concurrent processing of these jobs. The original design dumped these jobs into MySQL, given that these jobs had to be persisted indefinitely until a trigger was detected.\u003c/p\u003e","title":"Wrong Tool For The Job: Concurrent Queues with Aerospike"},{"content":"It all started with a deployment to the production cluster.\nIt always does. The worst things happen when you deploy to prod.\nBackground In our production cluster, we use Aerospike as the primary data store, with data being synced to MySQL for long term storage. For the uninitiated, Aerospike is a high speed, distributed key-value NoSQL database which provides a lot of cool features. Check it out if you haven\u0026rsquo;t already. In our cluster, all transactional data gets written to or read from AS, with MySQL being used only as a fallback option. We have a dedicated service that sync data from AS to MySQL and keeps things in check. The speed of access and the ability to scale by adding new nodes helps us keep the pressure off our central MySQL datastore.\nI was working on a project that migrated one of our legacy use cases from MySQL to Aerospike. Like all legacy software, this one had a bunch of implicit assumptions about the data store baked into the code, the primary one being persistence. A note about Aersopike - typically data are stored in Aerospike records with a TTL (Time To Live). The data gets evicted automatically by the Aerospike engine and this reduces a lot of manual garbage collection from our side. Sadly, this would not be a preferable trait for my use case, as the data was expected to be persisted for weeks or even months, while our typical TTL was about a week. Fortunately for me, AS provided a way to persist data indefinitely using -1 as the TTL. Yes! Problem solved. This was the least of my worries as I had to abuse Aersopike in ways that would make its creators cry. That is a story for another post.\nI made the required changes, tested out the code, and things seemed to have improved drastically. After a round of code review, I was ready for deployment. The deployment progressed as usual. The use-case was served by a set of APIs, so I was monitoring the cluster for 5xx or any usual errors. The whole thing was done in about 10 mins and all the signals from the cluster were green. No 5xx. No uncaught errors. I patted myself on the back for a smooth deployment (those seem to be a rarity in my life these days).\nA series of unfortunate events Remember all those movie characters who celebrated early and later gets killed? A similar, but much less gruesome fate awaited me.\nIt started with the load balancer throwing 5xx. On further investigation, I found that the backend instance was not responding to certain requests. Digging deeper and grepping through the logs, I saw that request processing for one of the APIs for a completely different use-case was causing it. From the logs, it looked like the request was processed midway and then things abruptly stopped.\nWeird indeed.\nI did not have a lot of time as this was hitting production traffic (Blue-Green deployments, you say? We\u0026rsquo;ve never heard of it). So I quickly reverted the code to the previous stable version and dug deeper.\nDelving into the code, I saw that processing stopped abruptly at a point where we were inserting some data into Aersopike with a TTL of -1. A little bit of context here - our internal wrapper over the AS client library had put some checks in place to prevent people from persisting data forever (TTL = -1). Whenever someone passed in -1, it\u0026rsquo;d quietly change that into 7 days and pass it along to the AS library. This was abused in several places in our code base where -1 would be passed in since they expected the lib to put in some default value. This would not do for my case and I\u0026rsquo;d changed the wrapper to pass -1 as is to the underlying layers. The offending piece of code was one where -1 was being passed. So I narrowed down my search and tried calling our client wrapper with -1 on the instance. I was greeted with a Segmentation Fault from the underlying library. Ah ha! Problem solved!\nWell, not exactly. Why did I not get this bug while testing? Our deployments process is a little weird. We have a copy of each dependency stashed away in an S3 bucket which we pull during the deployment. I had used the same version of the lib during testing and the bug did not manifest for me. I dug even deeper (God, when will this stop?!!).\nOn checking the library version in one of the instances, I found that it had an older version of the lib installed. Suspecting something wrong with the deployment, I pulled up the deployment scripts, expecting to see something amiss. The Aerospike client deployment part was pretty straight forward:\n# script set up cd /usr/src wget https://s3.xyz.com/abc/aerospike_client.zip -O aerospike_client.zip unzip aerospike_client.zip cd \u0026lt;unzipped dir\u0026gt; # make and install Things looks right. Nothing out of place. I ran just the client deployment script and checked again. It was still showing the older version of the library!\nSuper werid.\nSomehow it dawned on me to check the man pages for unzip. Usually when you uzip a file and if the resulting directory already exists then unzip will prompt you regarding the next course of action. But when the same script is triggered through an ansible role, it\u0026rsquo;ll silently do nothing and move on with the rest of the flow. Therein lies the problem!\nThe base AMI we were using already had the unzipped folder baked in with the old version of the library. Whenever the deployment script ran, we downloaded the library code, and tried to unzip it. With no-one to tell it what to do, unzip silently did nothing. Not a single thing. Looking closer, I found that this had been the case since 2016. For 4 years, we had happily deployed code with not a single soul knowing that things were not being deployed as expected.\nThe fix was a simple addition of the -o flag to the unzip command so that it could pummel through anyone and anything that stood in its way.\nThis seemingly innocuous bug took me from high level application code, to Aerospike client library code, and then, down to our deployment script. All of this because someone did not explicitly instruct unzip to replace while extracting its contents.\nAll for want of a -o flag.\nLessons Learned Don\u0026rsquo;t deploy on a Friday. Have some heart and think about your on-call engineers. Things can blow up in your face. Be ready to log it when it happens. I had to manually test the client wrapper to find that it was a SEGFAULT. Don\u0026rsquo;t always assume the fault is in your code. Never blindly trust client libraries to do the right thing. We\u0026rsquo;re all human after all. Don\u0026rsquo;t put conflicting defaults in client wrapper code. Don\u0026rsquo;t be an idiot like me and try to change those defaults. Once out in the wild, every perceivable behavior of a lib will be (ab)used by programmers. Read the Frickin\u0026rsquo; Manual \u0026amp; Be EXPLICIT with your command. Bash has enough red tape around it as is. Make sure that your favorite tools behave the way you expect when you plug it into a script. Always err on the side of verbosity and add flags to ensure the expected behavior. Always be ready to dig further. You will most definitely end up learning a lot with a good story to boot. Ping me your thoughts and comments.\n","permalink":"//localhost:1313/posts/02-bug-ttl/","summary":"\u003cp\u003eIt all started with a deployment to the production cluster.\u003c/p\u003e\n\u003cp\u003eIt always does. The worst things happen when you deploy to prod.\u003c/p\u003e\n\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eIn our production cluster, we use Aerospike as the primary data store, with data being synced to MySQL for long term storage. For the uninitiated, Aerospike is a high speed, distributed key-value NoSQL database which provides a lot of cool features. Check it \u003ca href=\"https://www.aerospike.com/\"\u003eout\u003c/a\u003e if you haven\u0026rsquo;t already. In our cluster, all transactional data gets written to or read from AS, with MySQL being used only as a fallback option. We have a dedicated service that sync data from AS to MySQL and keeps things in check. The speed of access and the ability to scale by adding new nodes helps us keep the pressure off our central MySQL datastore.\u003c/p\u003e","title":"Bug Story: It's not you, it's the environment"},{"content":"Hello!\nIs there anybody out there?\nNod if you can hear me\nThis blog is my poor attempt to document all the werid bugs I have encountered in production and the valuable lessons they\u0026rsquo;ve taught me.\nIt took me 5 attempts just to get this page up and running. Hmm. There must be story here that I can stretch to a blog post.\n","permalink":"//localhost:1313/posts/01-hello-world/","summary":"\u003cp\u003eHello!\u003c/p\u003e\n\u003cp\u003eIs there anybody out there?\u003c/p\u003e\n\u003cp\u003eNod if you can hear me\u003c/p\u003e\n\u003cp\u003eThis blog is my poor attempt to document all the werid bugs I have encountered in production and the valuable lessons they\u0026rsquo;ve taught me.\u003c/p\u003e\n\u003cp\u003eIt took me 5 attempts just to get this page up and running. Hmm. There must be story here that I can stretch to a blog post.\u003c/p\u003e","title":"Hello World"}]